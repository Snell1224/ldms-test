#!/usr/bin/env python
from __future__ import print_function
import os
import re
import sys
import pwd
import json
import time
import docker
import argparse
import logging

import TADA

from LDMS_Test import LDMSDCluster, LDMSDContainer, \
                      add_common_args, process_args

logging.basicConfig(format = "%(asctime)s %(name)s %(levelname)s %(message)s",
                    level = logging.INFO)

log = logging.getLogger(__name__)

dc = docker.from_env()
USER = pwd.getpwuid(os.geteuid())[0]

spec = {
    "name" : "TO-BE-REPLACED",
    "description" : "spank_notifier_test cluster",
    "type" : "NA",
    "templates" : {
        "compute-node" : {
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "slurmd",
                    "!extends" : "slurmd-template",
                },
            ],
        },
        "slurmd-template" : {
            "type" : "slurmd",
            "plugstack" : [
                {
                    "required" : True,
                    "path" : "%libdir%/ovis-ldms/libslurm_notifier.so",
                    "args" : [
                        "auth=none",
                        "port=20000",
                        "timeout=1",
                    ],
                },
            ],
        },
    },
    "nodes" : [
        {
            "hostname" : "node-1",
            "!extends" : "compute-node",
        },
        {
            "hostname" : "node-2",
            "!extends" : "compute-node",
        },
        {
            "hostname" : "headnode",
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "slurmctld",
                    "type" : "slurmctld",
                },
            ],
        },
    ],

    "cap_add": [ "SYS_PTRACE" ],
    "image": "ovis-centos-build",
    "libdir": "/opt/ovis/lib64",
}

def make_dict_from_out(out):
    env = out.split('\n')
    d = {}
    for e in env:
        e = e.split('=')
        if len(e) > 1:
            d[e[0]] = e[1]
    return d

def remove_job_out(cluster):
    for cont in cluster.containers:
        rc, out = cont.exec_run("rm -f /db/slurm_env_{node}.out".format(node=cont.hostname))
        rc, out = cont.exec_run("rm -f /db/crash_env_{node}.out".format(node=cont.hostname))
        rc, out = cont.exec_run("rm -f /db/spank_stream_{node}.out".format(node=cont.hostname))

def get_mount_libdir(prefix, mount_prefix):
    if os.path.exists(prefix + "/lib64"):
        return mount_prefix + "/lib64"
    return mount_prefix + "/lib"

if __name__ == "__main__":
    if sys.flags.interactive:
        execfile(os.getenv("PYTHONSTARTUP", "/dev/null"))
    parser = argparse.ArgumentParser(description="Run an FVT test")
    add_common_args(parser)
    parser.add_argument("--libdir",
                        help="The directory where the test target is installed.",
                        default="__find_from_prefix__")
    args = parser.parse_args()
    process_args(args)

    COMMIT_ID = args.commit_id
    spec["name"] = args.clustername

    test = TADA.Test(test_suite = "Slurm_Plugins",
                     test_type = "FVT",
                     test_name = "spank_notifier_test",
                     test_desc = "The test to verify ldmsd-stream SPANK notifier.",
                     test_user = args.user,
                     commit_id = COMMIT_ID,
                     tada_addr=args.tada_addr)
    test.add_assertion(0, 'Missing stream listener on node-1 does not affect job execution')
    test.add_assertion(1, 'Missing stream listener on node-2 does not affect job execution')
    test.add_assertion(2, "first event on node-1 is 'init'")
    test.add_assertion(3, "'init' event on node-1 contains subscriber data")
    test.add_assertion(4, "second event on node-1 is 'task_init_priv'")
    test.add_assertion(5, "third event on node-1 is 'task_exit'")
    test.add_assertion(6, "fourth event on node-1 is 'exit'")
    test.add_assertion(7, "first event on node-2 is 'init'")
    test.add_assertion(8, "'init' event on node-2 contains subscriber data")
    test.add_assertion(9, "second event on node-2 is 'task_init_priv'")
    test.add_assertion(10, "third event on node-2 is 'task_exit'")
    test.add_assertion(11, "fourth event on node-2 is 'exit'")
    test.add_assertion(12, "Killing stream listener does not affect job execution on node-1")
    test.add_assertion(13, "Killing stream listener does not affect job execution on node-2")
    test.start()

    log.info("-- Create the cluster --")
    spec['ovis_prefix'] = args.prefix
    spec['mounts'] = [ args.data_root + ':/db:rw' ]
    if args.src:
        spec['mounts'].append("{src}:{src}:rw".format(src=args.src))
    # modify slurm_notifier path in spec
    if args.libdir == "__find_from_prefix__":
        args.libdir = get_mount_libdir(args.prefix, "/opt/ovis")
    spec["libdir"] = args.libdir
    try:
        cluster = LDMSDCluster.get(spec["name"], create=False, spec = spec)
        cluster.remove()
    except:
        pass
    cluster = LDMSDCluster.get(spec["name"], create=True, spec = spec)

    log.info("-- Start daemons --")
    cluster.start_daemons()

    # Test Strategy:
    #
    # 1. Run the test tool ldmsd_stream_subscriber to listen to stream
    #    data on the stream named "slurm" and writes the events to a
    #    file called 'spank_stream.out'
    #
    # 2. The spank notifier will write respond to the spank events by
    #    writing the job data to an LDMSD Stream called "slurm".
    #
    # 3. Run a job that writes the SLURM environment variables to a
    #    file called 'spank_test.out'
    #
    # 4. The assertions will confirm that the SLURM environment
    #    variables match the data received on the LDMSD Stream
    #

    log.info("-- Create the sbatch and job scripts --")
    cont = cluster.get_container("node-1")
    cont.write_file('/db/show_spank_env.sh',
                    '#!/bin/bash\n'\
                    'env | grep SLURM > /db/slurm_env_$(hostname).out\n')
    cont.write_file('/db/spank_job.sh',
                    '#!/bin/bash\n'\
                    '#SBATCH -N 2\n'\
                    '#SBATCH --output spank_test.out\n'\
                    '#SBATCH -D /db\n'\
                    'export SUBSCRIBER_DATA=\'{"sub":"data"}\'\n'\
                    'srun bash /db/show_spank_env.sh\n')

    # If there is no one listening for stream events, the job still runs.
    log.info("-- Submitting job with no stream listener --")
    remove_job_out(cluster)
    jobid = cluster.sbatch("/db/spank_job.sh")
    log.info("  jobid = {0}".format(jobid))
    time.sleep(10) # enough time for the job to run

    # Read the environment file to confirm that the job ran
    assert_no = 0
    for node in [ 'node-1', 'node-2' ]:
        cont = cluster.get_container(node)
        out = cont.read_file('/db/slurm_env_{node}.out'.format(node=cont.hostname))
        d = make_dict_from_out(out)
        test.assert_test(assert_no, 'SLURM_JOB_NAME' in d, 'job output file created')
        assert_no += 1

    # cleanup prior job's results
    remove_job_out(cluster)

    slurmd_containers = [cluster.get_container(c) for c in ["node-1","node-2"]]

    # start a stream subscriber
    for cont in slurmd_containers:
        rc, out = cont.exec_run("ldmsd_stream_subscribe -p 20000 "\
                                "-s slurm -f /db/spank_stream_{node}.out "\
                                "-D".format(node=cont.hostname))
        if rc != 0:
            log.info(out)
            cluster.remove()
            sys.exit(1)

    log.info("-- Submitting job with listener --")
    jobid = cluster.sbatch("/db/spank_job.sh")
    log.info("  jobid = {0}".format(jobid))
    time.sleep(3) # enough time for the job to run

    # Read the environment file to see that it ran
    node_env = []
    stream_data = []
    for cont in slurmd_containers:
        node_env.append(cont.read_file('/db/slurm_env_{node}.out'.format(node=cont.hostname)))
        stream_data.append(cont.read_file('/db/spank_stream_{node}.out'.format(node=cont.hostname)))

    # Parse the files and create dictionaries
    env_dict = []
    for env in node_env:
        env = env.split('\n')
        d = {}
        for e in env:
            e = e.split('=')
            if len(e) > 1:
                d[e[0]] = e[1]
        env_dict.append(d)

    node_events = []
    for events in stream_data:
        events = events.split('EVENT:')
        event_list = []
        for event in events:
            if len(event) == 0:
                continue
            d = json.loads(event)
            e = d['event']
            event_list.append(e)
        node_events.append(event_list)

    subscriber_data = {"sub": "data"}
    for node in range(0, 2):
        e = env_dict[node]
        events = node_events[node]
        test.assert_test(assert_no, events[0]['event'] == 'init',
                         '{0} == {1}'.format(events[0]['event'], 'init'))
        assert_no += 1
        test.assert_test(assert_no, events[0]['data']['subscriber_data'] == subscriber_data,
                         '{0} == {1}'.format(events[0]['data']['subscriber_data'], subscriber_data))
        assert_no += 1
        test.assert_test(assert_no, events[1]['event'] == 'task_init_priv',
                         '{0} == {1}'.format(events[1]['event'], 'task_init_priv'))
        assert_no += 1
        test.assert_test(assert_no, events[2]['event'] == 'task_exit',
                         '{0} == {1}'.format(events[2]['event'], 'task_exit'))
        assert_no += 1
        test.assert_test(assert_no, events[3]['event'] == 'exit',
                         '{0} == {1}'.format(events[3]['event'], 'exit'))
        assert_no += 1

    # Run a job that crashes the stream listener while the job is running
    cont = cluster.get_container("node-1")
    cont.write_file('/db/crash_stream_listener.sh',
                    '#!/bin/bash\n'\
                    'pkill ldmsd_stream_subscriber\n'\
                    'env | grep SLURM > /db/crash_env_$(hostname).out\n')
    cont.write_file('/db/spank_job_2.sh',
                    '#!/bin/bash\n'\
                    '#SBATCH -N 2\n'\
                    '#SBATCH --output spank_test.out\n'\
                    '#SBATCH -D /db\n'\
                    'export SUBSCRIBER_DATA=\'{"sub":"data"}\'\n'\
                    'srun bash /db/crash_stream_listener.sh\n')

    remove_job_out(cluster)

    log.info("-- Submitting job that crashes listener --")
    jobid = cluster.sbatch("/db/spank_job_2.sh")
    log.info("  jobid = {0}".format(jobid))
    time.sleep(3) # enough time for the job to run

    for node in [ 'node-1', 'node-2' ]:
        cont = cluster.get_container(node)
        out = cont.read_file('/db/crash_env_{node}.out'.format(node=cont.hostname))
        d = make_dict_from_out(out)
        test.assert_test(assert_no, 'SLURM_JOB_NAME' in d, 'job output file created')
        assert_no += 1

    test.finish()

    cluster.remove() # this destroys entire cluster
