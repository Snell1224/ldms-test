#!/usr/bin/env python3
from __future__ import print_function
import argparse
import os
import sys
import pwd
import TADA
import time
import json
import logging

from io import StringIO

from distutils.spawn import find_executable
from LDMS_Test import LDMSDCluster, D, process_args, add_common_args, read_msg

logging.basicConfig(format = "%(asctime)s %(name)s %(levelname)s %(message)s",
                    level = logging.INFO)

log = logging.getLogger(__name__)

spec = {
    "name" : "REPLACE_ME",
    "description" : "cluster for ldmsd_stream_test",
    "templates" : { # generic template can apply to any object by "!extends"
        "compute-node" : {
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "munged",
                    "type" : "munged",
                },
                {
                    "name" : "sampler-daemon",
                    "requires" : [ "munged" ],
                    "!extends" : "ldmsd-sampler",
                },
            ],
        },
        "ldmsd-sampler" : {
            "!extends" : "ldmsd-base",
            "samplers" : [
                {
                    "plugin" : "test_stream_sampler",
                    "interval" : 1000000,
                    "offset" : 0,
                    "config" : [
                        "component_id=%component_id%",
                        "instance=%hostname%/%plugin%",
                        "producer=%hostname%",
                        "stream=test_stream",
                        "output=/data/%hostname%.out"
                    ]
                },
            ],
        },
        "ldmsd-base" : {
            "type" : "ldmsd",
            "listen_port" : 10000,
            "listen_xprt" : "sock",
            "listen_auth" : "munge",
        },
        "prdcr" : {
            "host" : "%name%",
            "port" : 10000,
            "xprt" : "sock",
            "type" : "active",
            "interval" : 1000000,
        },
    },
    "nodes" : [
        {
            "hostname" : "stream-sampler-1",
            "component_id" : 10001,
            "!extends" : "compute-node",
        },
        {
            "hostname" : "stream-sampler-2",
            "component_id" : 10002,
            "!extends" : "compute-node",
        },
        {
            "hostname" : "agg-1",
            "component_id" : 10003,
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "munged",
                    "type" : "munged",
                },
                {
                    "name" : "aggregator",
                    "!extends" : "ldmsd-base",
                    "listen_port" : 20000, # override
                    "prdcrs" : [ # these producers will turn into `prdcr_add`
                        {
                            "name" : "stream-sampler-1",
                            "!extends" : "prdcr",
                        },
                        {
                            "name" : "stream-sampler-2",
                            "!extends" : "prdcr",
                        },
                    ],
                    "config" : [ # additional config applied after prdcrs
                        "prdcr_subscribe regex=.* stream=test_stream",
                        "prdcr_start_regex regex=.*",
                    ],
                },
            ]
        },
        {
            "hostname" : "agg-2",
            "component_id" : 10004,
            "daemons" : [
                {
                    "name" : "sshd",
                    "type" : "sshd",
                },
                {
                    "name" : "munged",
                    "type" : "munged",
                },
                {
                    "name" : "aggregator",
                    "!extends" : "ldmsd-base",
                    "listen_port" : 20000, # override
                    "samplers" : [
                        {
                            "plugin" : "test_stream_sampler",
                            "interval" : 1000000,
                            "offset" : 0,
                            "config" : [
                                "component_id=%component_id%",
                                "instance=%hostname%/%plugin%",
                                "producer=%hostname%",
                                "stream=test_stream",
                                "output=/data/%hostname%.out"
                            ]
                        },
                    ],
                    "prdcrs" : [ # these producers will turn into `prdcr_add`
                        {
                            "name" : "agg-1",
                            "!extends" : "prdcr",
                            "port" : 20000,
                        },
                    ],
                    "config" : [ # additional config applied after prdcrs
                        "prdcr_subscribe regex=.* stream=test_stream",
                        "prdcr_start_regex regex=.*",
                    ],
                },
            ]
        },
    ],

    #"image": "ovis-centos-build:slurm",
    "cap_add": [ "SYS_PTRACE" ],
    "image": "ovis-centos-build",
    "ovis_prefix": "REPLACE_ME",
    "env" : {
        "LD_LIBRARY_PATH": "/data/tada/lib:/opt/ovis/lib:/opt/ovis/lib64",
        "LDMSD_PLUGIN_LIBPATH": "/data/tada/lib:/opt/ovis/lib/ovis-ldms:/opt/ovis/lib64/ovis-ldms",
    },
    "mounts": [ "{}:/tada-src:ro".format(os.path.realpath(sys.path[0])) ],
}

def rm(path):
    if os.path.exists(path):
        os.remove(path)

if __name__ == "__main__":
    if sys.flags.interactive:
        exec(open(os.getenv("PYTHONSTARTUP", "/dev/null")).read())
    parser = argparse.ArgumentParser(description="ldmsd_stream_publish/subscribe FVT test")
    add_common_args(parser)
    args = parser.parse_args()
    process_args(args)

    clustername = args.clustername
    COMMIT_ID = args.commit_id

    spec["ovis_prefix"] = args.prefix
    spec["name"] = clustername
    spec["mounts"] += [ "{}:/data:rw".format(args.data_root) ]
    spec["mounts"] += args.mount
    if args.src:
        spec["mounts"] += [ "{0}:{0}:ro".format(args.src) ]
    spec["env"]["TADA_USER"] = args.user
    spec["env"]["TADA_ADDR"] = args.tada_addr

    # remove existing files
    rm(args.data_root + "/stream-sampler-1.out")
    rm(args.data_root + "/stream-sampler-2.out")
    rm(args.data_root + "/agg-2.out")

    # test = TADA.Test(args.cfg, args.prefix, args.data_root, args.tada_addr)
    test = TADA.Test(test_suite = "LDMSD",
                     test_type = "SVT",
                     test_name = "ldmsd_stream_test",
                     test_desc = "LDMSD stream system verification test",
                     test_user = args.user,
                     commit_id = COMMIT_ID,
                     tada_addr = args.tada_addr)
    test.add_assertion(0, 'ldmsd_stream_publish of JSON data to stream-sampler-1 succeeds')
    test.add_assertion(1, 'ldmsd_stream_publish of STRING data to stream-sampler-1 succeeds')
    test.add_assertion(2, 'ldmsd_stream_publish to JSON data to stream-sampler-2 succeeds')
    test.add_assertion(3, 'ldmsd_stream_publish of STRING data to stream-sampler-2 succeeds')
    test.add_assertion(4, 'ldmsd_stream data check on agg-2')

    test.add_assertion(5, 'Stopping the producers succeeds')
    test.add_assertion(6, 'Restarting the producers succeeds')

    test.add_assertion(7, 'JSON stream data resumes after producer restart on stream-sampler-1')
    test.add_assertion(8, 'STRING stream data resumes after producer rerestart on stream-sampler-1')
    test.add_assertion(9, 'JSON stream data resumes after producer restart on stream-sampler-2')
    test.add_assertion(10, 'STRING stream data resumes after producer rerestart on stream-sampler-2')
    test.add_assertion(11, 'ldmsd_stream data resume check on agg-2')

    test.add_assertion(12, 'stream-sampler-1 is not running')
    test.add_assertion(13, 'stream-sampler-1 has restarted')
    test.add_assertion(14, 'JSON stream data resumes after stream-sampler-1 restart')
    test.add_assertion(15, 'STRING stream data resumes after stream-sampler-1 restart')
    test.add_assertion(16, 'ldmsd_stream data check on agg-2 after stream-sampler-1 restart')

    # Tell the TADA infrastructure that the test is starting
    test.start()

    # Create the containers required to ruyn the test
    cluster = LDMSDCluster.get(clustername, create = True, spec = spec)

    # Build libtada and libtest_stream_sampler before starting ldmsd
    cont = cluster.get_container("agg-1")
    rc, out = cont.exec_run("make -C /tada-src/C BUILDDIR=/data/tada/lib")
    if rc:
        raise RuntimeError("libtada build failed, output: {}".format(out))

    # Start all the LDMS Daemons configured in each container. NB: LDMSD can also be started
    # individually with start_daemon('hostname')
    cluster.start_daemons()

    # Give the daemons a few seconds to start
    time.sleep(5)

    # Create the test data
    data = { "gen" : 1,
             "schema" : "stream_test",
             "timestamp" : 1559242264,
             "data" : {
                 "id" : 12345,
                 "list" : [ 1, 2, 3, 4 ]
             }
         }
    text_data = json.dumps(data)
    data_file = '/data/Stream_Test-data.json'

    assert_no = 0
    for host in [ 'stream-sampler-1', 'stream-sampler-2' ]:
        cont = cluster.get_container(host)
        cont.write_file(data_file, text_data)
        cmd = "bash -c \"" \
              "/data/tada/lib/test_stream_publish -h {host} -x {xprt} -p {port} -a {auth}" \
              " -s test_stream -t json -f {fname} & " \
              "/data/tada/lib/test_stream_publish -h {host} -x {xprt} -p {port} -a {auth}" \
              " -s test_stream -t string -f {fname} & " \
              "wait" \
              "\"".format( fname=data_file,
                           host=host,
                           xprt=cont.ldmsd_spec["listen_xprt"],
                           port=cont.ldmsd_spec["listen_port"],
                           auth=cont.ldmsd_spec["listen_auth"] )

        rc, out = cont.exec_run(cmd)

    # data verification
    samp1 = cluster.get_container("stream-sampler-1")
    samp2 = cluster.get_container("stream-sampler-2")
    agg2 = cluster.get_container("agg-2")

    samp1_out_txt = samp1.read_file("/data/stream-sampler-1.out")
    samp2_out_txt = samp2.read_file("/data/stream-sampler-2.out")
    agg2_out_txt = agg2.read_file("/data/agg-2.out")

    samp1_out = StringIO(samp1_out_txt)
    samp2_out = StringIO(samp2_out_txt)
    agg2_out = StringIO(agg2_out_txt)

    # Verify samp1
    for x in range(0, 2):
        msg = read_msg(samp1_out)
        if msg["type"] == "json":
            test.assert_test(0, msg["obj"] == data, "verify JSON data")
        elif msg["type"] == "string":
            test.assert_test(1, msg["text"] == text_data, "verify STRING data")
        else:
            raise RuntimeError("Bad message type")

    # Verify samp2
    for x in range(0, 2):
        msg = read_msg(samp2_out)
        if msg["type"] == "json":
            test.assert_test(2, msg["obj"] == data, "verify JSON data")
        elif msg["type"] == "string":
            test.assert_test(3, msg["text"] == text_data, "verify STRING data")
        else:
            raise RuntimeError("Bad message type")

    # Verify agg2
    text_count = 0
    json_count = 0
    _verify = True
    _verify_msg = "agg2 stream data verification"
    for x in range(0, 4):
        msg = read_msg(agg2_out)
        if msg["type"] == "string":
            text_count += 1
            _verify &= (msg["text"] == text_data)
        elif msg["type"] == "json":
            json_count += 1
            _verify &= (msg["obj"] == data)
        else:
            _verify = False
            _verify_msg = "Bad message type: {}".format(msg["type"])
    if _verify and (text_count != 2 or json_count != 2):
        _verify = False
        _verify_msg = "Message mismatch"
    test.assert_test(4, _verify, _verify_msg)

    agg_h = cluster.get_container('agg-1')
    rc, out = agg_h.config_ldmsd([ "prdcr_stop_regex regex=.*" ])
    test.assert_test(5, (len(out) == 0), out)

    rc, out = agg_h.config_ldmsd([ "prdcr_start_regex regex=.*" ])
    test.assert_test(6, (len(out) == 0), out)

    # give them time to reconnect before publishing new data
    time.sleep(5)

    # second set of data
    data = { "gen" : 2,
             "schema" : "stream_test",
             "timestamp" : 1559242274,
             "data" : {
                 "id" : 23456,
                 "list" : [ 1, 2, 3, 4, 5, 6 ]
             }
         }
    text_data = json.dumps(data)

    for host in [ 'stream-sampler-1', 'stream-sampler-2' ]:
        cont = cluster.get_container(host)
        cont.write_file(data_file, text_data)
        cmd = "bash -c \"" \
              "/data/tada/lib/test_stream_publish -h {host} -x {xprt} -p {port} -a {auth}" \
              " -s test_stream -t json -f {fname} & " \
              "/data/tada/lib/test_stream_publish -h {host} -x {xprt} -p {port} -a {auth}" \
              " -s test_stream -t string -f {fname} & " \
              "wait" \
              "\"".format( fname=data_file,
                           host=host,
                           xprt=cont.ldmsd_spec["listen_xprt"],
                           port=cont.ldmsd_spec["listen_port"],
                           auth=cont.ldmsd_spec["listen_auth"] )

        rc, out = cont.exec_run(cmd)

    samp1_out_txt2 = samp1.read_file("/data/stream-sampler-1.out")
    samp2_out_txt2 = samp2.read_file("/data/stream-sampler-2.out")
    agg2_out_txt2 = agg2.read_file("/data/agg-2.out")

    samp1_out = StringIO(samp1_out_txt2)
    samp2_out = StringIO(samp2_out_txt2)
    agg2_out = StringIO(agg2_out_txt2)

    samp1_out.seek(len(samp1_out_txt))
    samp2_out.seek(len(samp2_out_txt))
    agg2_out.seek(len(agg2_out_txt))

    # Verify samp1
    for x in range(0, 2):
        msg = read_msg(samp1_out)
        if msg["type"] == "json":
            test.assert_test(7, msg["obj"] == data, "verify JSON data")
        elif msg["type"] == "string":
            test.assert_test(8, msg["text"] == text_data, "verify STRING data")
        else:
            raise RuntimeError("Bad message type")

    # Verify samp2
    for x in range(0, 2):
        msg = read_msg(samp2_out)
        if msg["type"] == "json":
            test.assert_test(9, msg["obj"] == data, "verify JSON data")
        elif msg["type"] == "string":
            test.assert_test(10, msg["text"] == text_data, "verify STRING data")
        else:
            raise RuntimeError("Bad message type")

    # Verify agg2
    text_count = 0
    json_count = 0
    _verify = True
    _verify_msg = "agg2 stream data verification"
    for x in range(0, 4):
        msg = read_msg(agg2_out)
        if msg["type"] == "string":
            text_count += 1
            _verify &= (msg["text"] == text_data)
        elif msg["type"] == "json":
            json_count += 1
            _verify &= (msg["obj"] == data)
        else:
            _verify = False
            _verify_msg = "Bad message type: {}".format(msg["type"])
    if _verify and (text_count != 2 or json_count != 2):
        _verify = False
        _verify_msg = "Message mismatch"
    test.assert_test(11, _verify, _verify_msg)

    cont = cluster.get_container('stream-sampler-1')

    cont.kill_ldmsd()
    time.sleep(1)
    running = cont.pgrepc('ldmsd')
    test.assert_test(12, (running == False), '(running == False)')

    cont.start_ldmsd()
    time.sleep(1)
    running = cont.pgrepc('ldmsd')
    test.assert_test(13, (running == True), '(running == True)')

    time.sleep(5) # allow some time for prdcr to reconnect

    data = { "gen" : 3,
             "schema" : "stream_test",
             "timestamp" : 1559250000,
             "data" : {
                 "id" : 78910,
                 "list" : [ 1  ]
             }
         }
    text_data = json.dumps(data)

    for host in [ 'stream-sampler-1' ]:
        cont = cluster.get_container(host)
        cont.write_file(data_file, text_data)
        cmd = "bash -c \"" \
              "/data/tada/lib/test_stream_publish -h {host} -x {xprt} -p {port} -a {auth}" \
              " -s test_stream -t json -f {fname} & " \
              "/data/tada/lib/test_stream_publish -h {host} -x {xprt} -p {port} -a {auth}" \
              " -s test_stream -t string -f {fname} & " \
              "wait" \
              "\"".format( fname=data_file,
                           host=host,
                           xprt=cont.ldmsd_spec["listen_xprt"],
                           port=cont.ldmsd_spec["listen_port"],
                           auth=cont.ldmsd_spec["listen_auth"] )
        rc, out = cont.exec_run(cmd)

    # samp1 was reset
    samp1_out_txt3 = samp1.read_file("/data/stream-sampler-1.out")
    agg2_out_txt3 = agg2.read_file("/data/agg-2.out")

    samp1_out = StringIO(samp1_out_txt3)
    agg2_out = StringIO(agg2_out_txt3)

    agg2_out.seek(len(agg2_out_txt2))

    # Verify samp1
    for x in range(0, 2):
        msg = read_msg(samp1_out)
        if msg["type"] == "json":
            test.assert_test(14, msg["obj"] == data, "verify JSON data")
        elif msg["type"] == "string":
            test.assert_test(15, msg["text"] == text_data, "verify STRING data")
        else:
            raise RuntimeError("Bad message type")

    # Verify agg2
    text_count = 0
    json_count = 0
    for x in range(0, 2):
        msg = read_msg(agg2_out)
        if msg["type"] == "string":
            text_count += 1
            _verify &= (msg["text"] == text_data)
        elif msg["type"] == "json":
            json_count += 1
            _verify &= (msg["obj"] == data)
        else:
            _verify = False
            _verify_msg = "Bad message type: {}".format(msg["type"])
    if _verify and (text_count != 1 or json_count != 1):
        _verify = False
        _verify_msg = "Message mismatch"
    test.assert_test(16, _verify, _verify_msg)

    test.finish()
    # cleanup data
    flist = [ "agg-2.out", "stream-sampler-1.out", "stream-sampler-2.out",
              "Stream_Test-data.json", "tada/" ]
    flist = [ "/data/{}".format(x) for x in flist ]
    rm = "rm -rf " + ( " ".join(flist) )
    agg2.exec_run(rm)
    cluster.remove()
